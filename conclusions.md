# Conclusions

As suggested by our R^2 and prediction accuracy scores, our models were not very successful in predicting the similarity scores of song comaprisons. Our best-performing model was the AdaBoosted trees, with a testing accuracy of 26%. However, this model also exhibited charactristics of overfitting on the training set, which lessened our confidence in it's predictive ability. Generally, we found that tree-based models performed the best on the given dataset, with Random Forest being the second best performing model, with testing accuracy of 19%. While more complex models such as the Boosted Trees and Random Forest produced better results, we acknowledge that we are compromising interpretability of the model. For example, it is difficult for us to interprete from these two models how each feature is contributing to similarity scores. Suppose the linear regression have yielded performance statistics to AdaBoosted trees or Random Forest, we would have devoted more of our model tuning and analysis on parameters that appear the most significant in the linear regression model.

We attribute the difficulty in using feature differences to accurately predict similarity scores partly to a lack of understanding of what similarity scores represent and how they were generated. Importantly, as the linear regression later revealed, there appeared to be little correlation between the predictor variables, all the features from Spotify API, and similarity score, which was extraced from the Million Songs dataset. Therefore, after coming to this realization through modeling, we then focused on designing our own similarity metric to recommend songs, explaining the motivation behind the second approach to playlist generation, documented under that section. In driving this second model, we focused on building intuition into what our model is predicting. Instead of using a metric calculated by Last.fm, we created a score that was based on minimizing the difference of each song's Spotify-scraped features and the average value of each feature for all the songs of the base playlist. Suppose the purpose of this playlist generation algorithm was for use in production, this approach would also allow us to provide greater transparency to artisits and users on why some songs are judged as more similar to others.

# Future Work

In terms of the overall framework of the project, we would also be interested in seeing how models would perform should we turn the problem into one of categorization as opposed to regression. For instance, another version of the goal of the problem could have been training a model to predict 1 for inclusion into an existing playlist, and 0 otherwise. We would be interested to tackle the major question of how we can provide training data for such a categorization problem (for example, by examining pre-made playlists on Spotify), and how we can judge the accuracy of such a model (perhaps via a metric similar to the similarity score metric we developed).

In terms of features that may increase modeling accuracy, one major feature that the team is interested in including is an analysis of the sentiment of the lyrics of each song. In fact, we built the requisite code to perform this analysis, but was restrained by the amount of time lyrics processing would take, even while using VADER to process the text. By including sentiment analysis in our model, we believe we will be able to better match the moods between the base playlist and new potential songs. Furthermore, we could turn this feature into a user-input, allowing the user of our algorithm to indicate preference for, or weight, certain moods above others.

Finally, we are interested in digging into the nuts and bolts of how Last.fm generated the scores and features in the dataset that we used to train our data. As we reflected in our conclusions, we were interested in learning more about the metrics that were reported in the given dataset. For instance, how did Last.fm estimate "valence", which was aimed to describe music positiveness? We believe that by more rigorously examining how the features of our dataset were generated, we can detect biases in the dataset early on and think of solutions to counteract such bias instances.
