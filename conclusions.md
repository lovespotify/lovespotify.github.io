# Conclusions

As suggested by our R^2 and prediction accuracy scores, our models were not very successful in predicting the similarity scores of song comparisons. Our best-performing model was the AdaBoosted trees, with a testing accuracy of 26%. However, this model also exhibited characteristics of overfitting on the training set, which lessened our confidence in its predictive ability. Generally, we found that tree-based models performed the best on the given dataset, with Random Forest being the second best performing model, with testing accuracy of 19%. While more complex models such as the Boosted Trees and Random Forest produced better results, we acknowledge that we are compromising interpretability of the model. For example, it is difficult for us to interpret from these two models how each feature is contributing to similarity scores. Suppose the linear regression have yielded performance statistics to AdaBoosted trees or Random Forest, we would have devoted more of our model tuning and analysis on parameters that appear the most significant in the linear regression model.

We attribute the difficulty in using feature differences to accurately predict similarity scores partly to a lack of understanding of what similarity scores represent and how they were generated. Importantly, as the linear regression later revealed, there appeared to be little correlation between the predictor variables, all the features from Spotify API, and similarity score, which was extracted from the Million Songs dataset. Therefore, after coming to this realization through modeling, we then focused on designing our own similarity metric to recommend songs, explaining the motivation behind the second approach to playlist generation, documented under that section. In driving this second model, we focused on building intuition into what our model is predicting. Instead of using a metric calculated by Last.fm, we created a score that was based on minimizing the difference of each song's Spotify-scraped features and the average value of each feature for all the songs of the base playlist. Suppose the purpose of this playlist generation algorithm was for use in production, this approach would also allow us to provide greater transparency to artisits and users on why some songs are judged as more similar to others.

We ran our model to provide two playlists of 50 songs on two different base playlists selected from the Million Playlist dataset. The base playlists, as well as the resulting recommended playlists given by the two different methods we used, can be seen in more detail in [Playlist Generation](https://lovespotify.github.io/playlistgeneration). However, as we discussed when presenting these playlists, it is difficult to find a significant degree of similarity or association between our base playlists and the produced recommendations even when we are using our own similarity metric and not the similarity score provided by Last.fm. This is representative of the low testing accuracy that we received during the training of our model, and raised many questions for us as to the data input into training our model.

# Future Work

In terms of the overall framework of the project, we would also be interested in seeing how models would perform should we turn the problem into one of categorization as opposed to regression. For instance, another version of the goal of the problem could have been training a model to predict 1 for inclusion into an existing playlist, and 0 otherwise. We would be interested in tackling the major question of how we can provide training data for such a categorization problem (for example, by examining pre-made playlists on Spotify), and how we can judge the accuracy of such a model (perhaps via a metric similar to the similarity score metric we developed).

In terms of features that may increase modeling accuracy, one major feature that the team is interested in including is an analysis of the sentiment of the lyrics of each song. In fact, we wrote up the requisite code to perform this analysis using natural language processing through VADER (Valence Aware Dictionary and sEntiment Reasoner), but we were restrained by the amount of time lyrics processing would take. By including sentiment analysis in our model, we believe we will be able to better match the moods between the base playlist and new potential songs. Furthermore, we could turn this feature into a form of user-input, allowing the user of our algorithm to indicate preference for, or weigh, certain moods above others.

We are interested in digging into the nuts and bolts of how Last.fm generated the similarity scores in the dataset that we used to train our data, as well as the values provided from the Spotify API for the audio features. As we reflected in our conclusions, we were interested in learning more about the metrics that were reported in the given dataset. For instance, how did Spotify estimate `valence`, which was aimed to describe music positiveness? We believe that by more rigorously examining how the features of our dataset were generated, we can detect biases in the dataset early on and think of solutions to counteract such bias instances.

From the results given from our two recommendation methods, we could find that there were some very strange songs recommended given the original base playlist. After some consideration, we concluded that if we had employed further filtering or added variables that could represent `genre` and `language`, that would likely help immensely in cutting out a lot of the songs that we found in the resulting recommendations. Unfortunately, the Spotify API does not yet have the ability to match songs to language, and genre is associated only with artists and not tracks, which could be misleading for artists who delve into many different genres. Perhaps, had we had a better understanding of the similarity scores extracted from the Million Songs dataset, another method we could have taken would be to base our model primarily on the Million Playlist dataset instead of relying on the similarity scores from Last.fm as we did in this model. There are many different routes that can be explored in order to further improve this model and ensure that it provides the songs that the user would like to hear.
